请注意，本章中描述的监督学习方法都需要安装 Numerical Python (numpy)。

## 16.1 逻辑回归模型

### 16.1.1 背景和目的

逻辑回归是一种监督学习方法，它试图用一些预测变量$x_i$的加权和来区分K类。逻辑回归模型是用来计算预测变量的权重$β_i$的。在Biopython中，逻辑回归模型目前只针对两个类（K = 2）实现；预测变量的数量没有预定的限制。

作为一个例子，让我们尝试预测细菌中的操作子结构。操作子是DNA同一条链上的一组相邻基因，这些基因被转录成一个mRNA分子。然后，单个mRNA分子的翻译产生了单个蛋白质。对于枯草杆菌（我们将使用其数据），一个操作子中的平均基因数约为2.4。

作为了解细菌基因调控的第一步，我们需要知道操作子的结构。对于枯草杆菌中大约10%的基因，从实验中可以知道操作子的结构。有监督的学习方法可以用来预测其余90%的基因的操作子结构。

对于这样的监督学习方法，我们需要选择一些容易测量的预测变量$x_i$，并与操纵子结构有一定的联系。一个预测变量可能是基因之间以碱基对为单位的距离。属于同一操作子的相邻基因往往相隔较短的距离，而不同操作子的相邻基因之间往往有较大的空间，以允许启动子和终止子序列的存在。另一个预测变量是基于基因表达的测量。根据定义，属于同一操作子的基因具有相等的基因表达谱，而不同操作子中的基因预计会有不同的表达谱。实际上，由于测量误差的存在，同一操作子中的基因的测量表达谱并不完全相同。

为了评估基因表达谱的相似性，我们假设测量误差遵循正态分布，并计算出相应的对数可能性分数。

我们现在有两个预测变量 ：我们可以用它来预测DNA同一链上的两个相邻基因是否属于同一操作子：

- $x_1$：它们之间的碱基对数；
- $x_2$：它们在表达谱上的相似性。

在逻辑回归模型中，我们使用这两个预测变量的加权和来计算一个联合得分S：
$$
S = β_0 + β_1 x_1 + β_2 x_2
$$
(16.1)

逻辑回归模型用两组实例基因给我们提供了参数β0、β1、β2的合适值:

- OP：相邻的基因，在DNA的同一条链上，已知属于同一操作子；
- NOP：相邻的基因，在DNA的同一条链上，已知属于不同的操作子。

在逻辑回归模型中，属于一个类别的概率通过逻辑函数取决于得分。对于OP和NOP这两个类别，我们可以写为
$$
Pr(OP|x_1,x_2)=\frac{exp(\beta_0+\beta_1x_1+\beta_2x_2)}{1+exp(\beta_0 + \beta_1x_1+\beta_2x_2)}(16.2)
$$

$$
Pr(NOP|x_1,x_2)=\frac{1}{1+exp(\beta_0 + \beta_1x_1+\beta_2x_2)}(16.3)
$$

使用一组已知属于同一操作子（OP类）或不同操作子（NOP类）的基因对，我们可以通过最大化概率函数（[16.2](http://biopython.org/DIST/docs/tutorial/Tutorial.html#eq%3AOP)）和（[16.3](http://biopython.org/DIST/docs/tutorial/Tutorial.html#eq%3ANOP)）所对应的对数可能性来计算权重β0、β1、β2。

### 16.1.2 训练逻辑回归模型

---

Table 16.1: Adjacent gene pairs known to belong to the same operon (class OP) or to different operons (class NOP). Intergene distances are negative if the two genes overlap.

表 16.1：已知属于同一操作子（OP类）或不同操作子（NOP类）的相邻基因对。如果两个基因重叠，基因间的距离为负数。

Gene pair：基因对；

Intergene distance (*x*1)：基因间距离 ( *x* 1 )  

Gene expression score (*x*2)：基因表达得分 ( *x* 2 )  类

Class：类

| Gene pair         | Intergene distance (*x*1) | Gene expression score (*x*2) | Class |
| ----------------- | ------------------------- | ---------------------------- | ----- |
| *cotJA* — *cotJB* | -53                       | -200.78                      | OP    |
| *yesK* — *yesL*   | 117                       | -267.14                      | OP    |
| *lplA* — *lplB*   | 57                        | -163.47                      | OP    |
| *lplB* — *lplC*   | 16                        | -190.30                      | OP    |
| *lplC* — *lplD*   | 11                        | -220.94                      | OP    |
| *lplD* — *yetF*   | 85                        | -193.94                      | OP    |
| *yfmT* — *yfmS*   | 16                        | -182.71                      | OP    |
| *yfmF* — *yfmE*   | 15                        | -180.41                      | OP    |
| *citS* — *citT*   | -26                       | -181.73                      | OP    |
| *citM* — *yflN*   | 58                        | -259.87                      | OP    |
| *yfiI* — *yfiJ*   | 126                       | -414.53                      | NOP   |
| *lipB* — *yfiQ*   | 191                       | -249.57                      | NOP   |
| *yfiU* — *yfiV*   | 113                       | -265.28                      | NOP   |
| *yfhH* — *yfhI*   | 145                       | -312.99                      | NOP   |
| *cotY* — *cotX*   | 154                       | -213.83                      | NOP   |
| *yjoB* — *rapA*   | 147                       | -380.85                      | NOP   |
| *ptsI* — *splA*   | 93                        | -291.13                      | NOP   |


------

表[16.1](http://biopython.org/DIST/docs/tutorial/Tutorial.html#table%3Atraining)列出了一些操作子结构已知的枯草芽孢杆菌（*Bacillus subtilis*）基因对。让我们根据这些数据计算出逻辑回归模型：

```python
>>> from Bio import LogisticRegression
>>> xs = [
...     [-53, -200.78],
...     [117, -267.14],
...     [57, -163.47],
...     [16, -190.30],
...     [11, -220.94],
...     [85, -193.94],
...     [16, -182.71],
...     [15, -180.41],
...     [-26, -181.73],
...     [58, -259.87],
...     [126, -414.53],
...     [191, -249.57],
...     [113, -265.28],
...     [145, -312.99],
...     [154, -213.83],
...     [147, -380.85],
...     [93, -291.13],
... ]
>>> ys = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
>>> model = LogisticRegression.train(xs, ys)
```

这里，`xs`和`ys`是训练数据：`xs`包含每个基因对的预测变量，`ys`指定该基因对是否属于同一操作子（`1`，类OP）或不同操作子（`0`，类NOP）。得到的逻辑回归模型被存储在`model`中，其中包含权重β0、β1和β2：

```python
>>> model.beta
[8.9830290157144681, -0.035968960444850887, 0.02181395662983519]
```

请注意，β1是负的，因为基因间距离较短的基因对属于同一操作子（OP类）的概率较大。另一方面，β2是正的，因为属于同一操作子的基因对通常有较高的基因表达谱的相似性分数。参数β0是正的，因为在训练数据中，操作子基因对比非操作子基因对的普遍性更高。

函数`train`有两个可选参数：`update_fn`和`typecode`。`update_fn`可以用来指定一个回调函数，把迭代次数和对数可能性作为参数。通过回调函数，我们可以跟踪模型计算的进度（使用Newton-Raphson迭代来最大化逻辑回归模型的对数似然函数）：

```python
>>> def show_progress(iteration, loglikelihood):
...     print(f"Iteration: {iteration} Log-likelihood function: {loglikelihood}")
...
>>>
>>> model = LogisticRegression.train(xs, ys, update_fn=show_progress)
Iteration: 0 Log-likelihood function: -11.7835020695
Iteration: 1 Log-likelihood function: -7.15886767672
Iteration: 2 Log-likelihood function: -5.76877209868
Iteration: 3 Log-likelihood function: -5.11362294338
Iteration: 4 Log-likelihood function: -4.74870642433
Iteration: 5 Log-likelihood function: -4.50026077146
Iteration: 6 Log-likelihood function: -4.31127773737
Iteration: 7 Log-likelihood function: -4.16015043396
Iteration: 8 Log-likelihood function: -4.03561719785
Iteration: 9 Log-likelihood function: -3.93073282192
Iteration: 10 Log-likelihood function: -3.84087660929
Iteration: 11 Log-likelihood function: -3.76282560605
Iteration: 12 Log-likelihood function: -3.69425027154
Iteration: 13 Log-likelihood function: -3.6334178602
Iteration: 14 Log-likelihood function: -3.57900855837
Iteration: 15 Log-likelihood function: -3.52999671386
Iteration: 16 Log-likelihood function: -3.48557145163
Iteration: 17 Log-likelihood function: -3.44508206139
Iteration: 18 Log-likelihood function: -3.40799948447
Iteration: 19 Log-likelihood function: -3.3738885624
Iteration: 20 Log-likelihood function: -3.3423876581
Iteration: 21 Log-likelihood function: -3.31319343769
Iteration: 22 Log-likelihood function: -3.2860493346
Iteration: 23 Log-likelihood function: -3.2607366863
Iteration: 24 Log-likelihood function: -3.23706784091
Iteration: 25 Log-likelihood function: -3.21488073614
Iteration: 26 Log-likelihood function: -3.19403459259
Iteration: 27 Log-likelihood function: -3.17440646052
Iteration: 28 Log-likelihood function: -3.15588842703
Iteration: 29 Log-likelihood function: -3.13838533947
Iteration: 30 Log-likelihood function: -3.12181293595
Iteration: 31 Log-likelihood function: -3.10609629966
Iteration: 32 Log-likelihood function: -3.09116857282
Iteration: 33 Log-likelihood function: -3.07696988017
Iteration: 34 Log-likelihood function: -3.06344642288
Iteration: 35 Log-likelihood function: -3.05054971191
Iteration: 36 Log-likelihood function: -3.03823591619
Iteration: 37 Log-likelihood function: -3.02646530573
Iteration: 38 Log-likelihood function: -3.01520177394
Iteration: 39 Log-likelihood function: -3.00441242601
Iteration: 40 Log-likelihood function: -2.99406722296
Iteration: 41 Log-likelihood function: -2.98413867259
```

一旦对数似然函数的增量小于 0.01，迭代就会停止。如果在 500 次迭代后仍未达到收敛，则`train`函数返回一个`AssertionError`.

可选的关键字`typecode`几乎都可以忽略不计。该关键字允许用户选择要使用的Numeric矩阵的类型。特别是，为了避免非常大的内存问题，可能有必要使用单精度的浮点数（Float8、Float16等）而不是默认使用的双倍数。

### 16.1.3 使用逻辑回归模型进行分类

分类是通过调用`classify`函数进行的。给定一个逻辑回归模型以及x1和x2的值（例如，对于一个未知操作子结构的基因对），分类函数返回1或0，分别对应于OP类和NOP类。例如，让我们考虑基因对*yxcE*, *yxcD*和*yxiB*, *yxiA*：

---

Table 16.2: Adjacent gene pairs of unknown operon status.

表16.2：未知操作子状态的相邻基因对。

| Gene pair       | Intergene distance *x*1 | Gene expression score *x*2 |
| --------------- | ----------------------- | -------------------------- |
| *yxcE* — *yxcD* | 6                       | -173.143442352             |
| *yxiB* — *yxiA* | 309                     | -271.005880394             |

------

逻辑回归模型将*yxcE*、*yxcD*分类为属于同一操纵子（类 OP），而预测*yxiB*、*yxiA*属于不同的操纵子：

```python
>>> print("yxcE, yxcD:", LogisticRegression.classify(model, [6, -173.143442352]))
yxcE, yxcD: 1
>>> print("yxiB, yxiA:", LogisticRegression.classify(model, [309, -271.005880394]))
yxiB, yxiA: 0
```

（顺便说一下，这与生物学文献一致）。

为了找出我们对这些预测的信心，我们可以调用`calculate`函数来获得OP类和NOP类的概率（公式（[16.2](http://biopython.org/DIST/docs/tutorial/Tutorial.html#eq%3AOP)）和[16.3](http://biopython.org/DIST/docs/tutorial/Tutorial.html#eq%3ANOP)）。对于*yxcE*，*yxcD*，我们发现

```python
>>> q, p = LogisticRegression.calculate(model, [6, -173.143442352])
>>> print("class OP: probability =", p, "class NOP: probability =", q)
class OP: probability = 0.993242163503 class NOP: probability = 0.00675783649744
```

对于*yxiB* , *yxiA*

```python
>>> q, p = LogisticRegression.calculate(model, [309, -271.005880394])
>>> print("class OP: probability =", p, "class NOP: probability =", q)
class OP: probability = 0.000321211251817 class NOP: probability = 0.999678788748
```

为了了解逻辑回归模型的预测准确性，我们可以将其应用于训练数据：

```python
>>> for i in range(len(ys)):
...     print(f"True: {ys[i]} Predicted: {LogisticRegression.classify(model, xs[i])}")
...
True: 1 Predicted: 1
True: 1 Predicted: 0
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
```

表明除了一个基因对之外，其他的预测都是正确的。可以从留一法分析中找到更可靠的预测准确度估计，其中在删除要预测的基因后从训练数据中重新计算模型：

```python
>>> for i in range(len(ys)):
...     model = LogisticRegression.train(xs[:i] + xs[i + 1 :], ys[:i] + ys[i + 1 :])
...     print(f"True: {ys[i]} Predicted: {LogisticRegression.classify(model, xs[i])}")
...
True: 1 Predicted: 1
True: 1 Predicted: 0
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 1
True: 0 Predicted: 0
True: 0 Predicted: 0
```

留一法分析表明，逻辑回归模型的预测只有两个基因对是不正确的，这相当于预测准确率为88%。

### 16.1.4 逻辑回归、线性判别分析和支持向量机

逻辑回归模型与线性判别分析相似。在线性判别分析中，类别概率也遵循公式（[16.2](http://biopython.org/DIST/docs/tutorial/Tutorial.html#eq%3AOP)）和（[16.3](http://biopython.org/DIST/docs/tutorial/Tutorial.html#eq%3ANOP)）。然而，我们不是直接估计系数β，而是首先对预测变量x进行正态分布的拟合，然后根据正态分布的平均值和协方差计算系数β。如果x的分布确实是正态的，那么我们期望线性判别分析比逻辑回归模型的表现更好。另一方面，逻辑回归模型对偏离正态的情况更为稳健。

另一种类似的方法是带有线性核的支持向量机。这样的支持向量机SVM也使用预测因子的线性组合，但从预测变量x在类之间的边界区域附近估计系数β。如果逻辑回归模型（方程（[16.2](http://biopython.org/DIST/docs/tutorial/Tutorial.html#eq%3AOP)）和（[16.3](http://biopython.org/DIST/docs/tutorial/Tutorial.html#eq%3ANOP)））对远离边界区域的x有很好的描述，我们希望逻辑回归模型比线性核的SVM表现更好，因为它依赖于更多的数据。如果不是这样，带线性核的SVM可能表现更好。

Trevor Hastie, Robert Tibshirani, and Jerome Friedman: *The Elements of Statistical Learning. Data Mining, Inference, and Prediction*. Springer Series in Statistics, 2001. Chapter 4.4.

## 16.2  *k* - 近邻法

### 16.2.1 背景和目的

k-近邻法是一种监督学习方法，不需要对数据进行模型拟合。相反，数据点的分类是基于训练数据集中的k个最近邻居的类别。

在Biopython中，k-nearest neighbors方法在`Bio.kNN`中可用。为了说明k-近邻方法在Biopython中的应用，我们将使用与[16.1](http://biopython.org/DIST/docs/tutorial/Tutorial.html#sec%3ALogisticRegression)节中相同的操纵子数据集。

### 16.2.2 初始化*k-*最近邻模型

使用[表16.1](http://biopython.org/DIST/docs/tutorial/Tutorial.html#table%3Atraining)中的数据，我们创建并初始化一个K-近邻模型，具体如下：

```python
>>> from Bio import kNN
>>> k = 3
>>> model = kNN.train(xs, ys, k)
```

其中`xs`和`ys`与[16.1.2节](http://biopython.org/DIST/docs/tutorial/Tutorial.html#sec%3ALogisticRegressionTraining)中的相同。这里，k是分类时要考虑的邻居k的数量。对于分为两类的分类，为k选择一个奇数可以避免平票（译者注：某个样本距离的偶数个邻居，正好平均分布在两类中，导致无法分类的情况）。函数名称`train`有点名不副实，因为没有进行模型训练：这个函数只是将`xs`、`ys`和`k`存储在`model`中。

### 16.2.3 使用*k*最近邻模型进行分类

为了使用K-近邻模型对新数据进行分类，我们使用`classify`函数。这个函数接收一个数据点（x1,x2）并找到训练数据集`xs`中的k个最近的邻居。然后根据哪个类别（`ys`）在k个邻居中出现最多，对数据点（x1, x2）进行分类。

对于基因对*yxcE*, *yxcD*和*yxiB*, *yxiA*的例子，我们发现：

```python
>>> x = [6, -173.143442352]
>>> print("yxcE, yxcD:", kNN.classify(model, x))
yxcE, yxcD: 1
>>> x = [309, -271.005880394]
>>> print("yxiB, yxiA:", kNN.classify(model, x))
yxiB, yxiA: 0
```

与逻辑回归模型一致，*yxcE*, *yxcD*被分类为属于同一个操作子（OP类），而*yxiB*, *yxiA*被预测为属于不同的操作子。

`classify`函数让我们指定一个距离函数和一个权重函数作为可选参数。距离函数会影响哪些k个邻居被选为最近的邻居，因为这些邻居被定义为与查询点（x，y）距离最小的邻居。默认情况下，使用的是欧氏距离。相反，我们可以使用城市街区（曼哈顿）距离：

```python
>>> def cityblock(x1, x2):
...     assert len(x1) == 2
...     assert len(x2) == 2
...     distance = abs(x1[0] - x2[0]) + abs(x1[1] - x2[1])
...     return distance
...
>>> x = [6, -173.143442352]
>>> print("yxcE, yxcD:", kNN.classify(model, x, distance_fn=cityblock))
yxcE, yxcD: 1
```

`weight`函数可用于加权投票。例如，我们可能想让较近的邻居比较远的邻居拥有更高的权重：

```python
>>> def weight(x1, x2):
...     assert len(x1) == 2
...     assert len(x2) == 2
...     return exp(-abs(x1[0] - x2[0]) - abs(x1[1] - x2[1]))
...
>>> x = [6, -173.143442352]
>>> print("yxcE, yxcD:", kNN.classify(model, x, weight_fn=weight))
yxcE, yxcD: 1
```

默认情况下，所有的邻居都被赋予同等的权重。

为了找出我们对这些预测的信心，我们可以调用`calculate`函数，它将计算分配给OP和NOP类的总权重。对于默认的加权方案，这可以简化为每个类别中的邻居数量。对于*yxcE*, *yxcD*，我们发现

```python
>>> x = [6, -173.143442352]
>>> weight = kNN.calculate(model, x)
>>> print("class OP: weight =", weight[0], "class NOP: weight =", weight[1])
class OP: weight = 0.0 class NOP: weight = 3.0
```

这意味着`x1`, `x2`的所有三个邻居都在NOP类中。再比如，对于*yesK*，*yesL*，我们发现

```python
>>> x = [117, -267.14]
>>> weight = kNN.calculate(model, x)
>>> print("class OP: weight =", weight[0], "class NOP: weight =", weight[1])
class OP: weight = 2.0 class NOP: weight = 1.0
```

这意味着两个邻居是操作子对，一个邻居是非操作子对。

为了了解k-nearest neighbors方法的预测准确性，我们可以将其应用于训练数据：

```python
>>> for i in range(len(ys)):
...     print(f"True: {ys[i]} Predicted: {kNN.classify(model, xs[i])}")
...
True: 1 Predicted: 1
True: 1 Predicted: 0
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
```

显示除了两个基因对之外，所有的预测都是正确的。一个更可靠的预测准确率的估计可以从留一法分析中找到，即在去除要预测的基因后，根据训练数据重新计算模型：

```python
>>> k = 3
>>> for i in range(len(ys)):
...     model = kNN.train(xs[:i] + xs[i + 1 :], ys[:i] + ys[i + 1 :], k)
...     print(f"True: {ys[i]} Predicted: {kNN.classify(model, xs[i])}")
...
True: 1 Predicted: 1
True: 1 Predicted: 0
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 1
True: 1 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 1
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 0
True: 0 Predicted: 1
```

留一法分析显示，K-邻近模型对17个基因对中的13个是正确的，对应的预测准确率为76%。

## 16.3 朴素贝叶斯

本节将介绍该`Bio.NaiveBayes`模块。

## 16.4 最大熵

本节将介绍该`Bio.MaxEntropy`模块。

## 16.5马尔可夫模型

本节将描述`Bio.MarkovModel`和/或`Bio.HMM.MarkovModel`模块。