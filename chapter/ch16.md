请注意，本章中描述的监督学习方法都需要安装 Numerical Python (numpy)。

# 16.1 逻辑回归模型



## 16.1.1 背景和目的

逻辑回归是一种监督学习方法，它尝试使用一些预测变量*xi**的*加权和来区分*K*个类。逻辑回归模型用于计算预测变量的权重β *i 。**在 Biopython 中，逻辑回归模型目前仅针对两个类（ K* = 2）实现；预测变量的数量没有预定义的限制。

例如，让我们尝试预测细菌中的操纵子结构。操纵子是同一 DNA 链上的一组相邻基因，它们被转录成单个 mRNA 分子。单个 mRNA 分子的翻译然后产生单个蛋白质。对于*枯草芽孢杆菌*，我们将使用其数据，操纵子中的平均基因数约为 2.4。

作为了解细菌基因调控的第一步，我们需要了解操纵子结构。*对于枯草芽孢杆菌*中约 10% 的基因，操纵子结构已从实验中获知。监督学习方法可用于预测剩余 90% 基因的操纵子结构。

对于这种监督学习方法，我们需要选择一些预测变量*x* *i*这可以很容易地测量，并且在某种程度上与操纵子结构有关。一个预测变量可能是基因之间碱基对的距离。属于同一操纵子的相邻基因倾向于相隔较短的距离，而不同操纵子中的相邻基因往往在它们之间具有较大的空间以允许启动子和终止子序列。另一个预测变量基于基因表达测量。根据定义，属于同一操纵子的基因具有相同的基因表达谱，而不同操纵子中的基因预计具有不同的表达谱。实际上，由于存在测量误差，同一操纵子中基因的测量表达谱并不完全相同。为了评估基因表达谱的相似性，

我们现在有两个预测变量，我们可以使用它们来预测同一 DNA 链上的两个相邻基因是否属于同一操纵子：

- *x* 1：它们之间的碱基对数；
- *x* 2：它们在表达谱上的相似性。

在逻辑回归模型中，我们使用这两个预测变量的加权和来计算联合得分*S*：

*S* = β 0 + β 1 *x* 1 + β 2 *x* 2。(16.1)

逻辑回归模型使用两组示例基因为我们提供了参数 β 0、 β 1、 β 2的适当值：

- OP：相邻基因，位于同一条 DNA 链上，已知属于同一操纵子；
- NOP：相邻基因，在同一条 DNA 链上，已知属于不同的操纵子。

在逻辑回归模型中，属于某个类别的概率取决于逻辑函数的得分。对于 OP 和 NOP 这两个类，我们可以将其写为

|      | Pr( *OP* \| *x* 1 ,  *x* 2 ) = exp(β 0 + β 1 *x* 1 + β 2 *x* 2 )1+exp(β 0 + β 1 *x* 1 + β 2 *x* 2 )  ‍ (16.2)Pr( *NOP* \| *x* 1 ,  *x* 2 ) = 1个1+exp(β 0 + β 1 *x* 1 + β 2 *x* 2 )  ‍(16.3) |
| ---- | ------------------------------------------------------------ |
|      |                                                              |

使用一组已知它们属于相同操纵子（OP 类）还是不同操纵子（NOP 类）的基因对，我们可以通过最大化对数似然来计算权重 β 0 、β 1 、 β 2对应于概率函数 ( [16.2](https://biopython-org.translate.goog/DIST/docs/tutorial/Tutorial.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN&_x_tr_pto=wapp&_x_tr_sch=http#eq%3AOP) ) 和 ( [16.3](https://biopython-org.translate.goog/DIST/docs/tutorial/Tutorial.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN&_x_tr_pto=wapp&_x_tr_sch=http#eq%3ANOP) )。

## 16.1.2 训练逻辑回归模型



> ------
>
> 表 16.1：已知属于同一操纵子（OP 类）或不同操纵子（NOP 类）的相邻基因对。如果两个基因重叠，则基因间距离为负。
>
> | 基因对            | 基因间距离 ( *x* 1 ) | 基因表达得分 ( *x* 2 ) | 班级 |
> | ----------------- | -------------------- | ---------------------- | ---- |
> | *cotJA* — *cotJB* | -53                  | -200.78                | OP   |
> | *是K-*是*L*       | 117                  | -267.14                | OP   |
> | *lplA* — *lplB*   | 57                   | -163.47                | OP   |
> | *lplB* — *lplC*   | 16                   | -190.30                | OP   |
> | *lplC* — *lplD*   | 11                   | -220.94                | OP   |
> | *lplD* — *yetF*   | 85                   | -193.94                | OP   |
> | *yfmT* — *yfmS*   | 16                   | -182.71                | OP   |
> | *yfmF* — *yfmE*   | 15                   | -180.41                | OP   |
> | *citS* —— *citT*  | -26                  | -181.73                | OP   |
> | *citM* — *yflN*   | 58                   | -259.87                | OP   |
> | *yfiI* — *yfiJ*   | 126                  | -414.53                | NOP  |
> | *lipB* — *yfiQ*   | 191                  | -249.57                | NOP  |
> | *yfiU* — *yfiV*   | 113                  | -265.28                | NOP  |
> | *yfhH* — *yfhI*   | 145                  | -312.99                | NOP  |
> | *cotY* —— *cotX*  | 154                  | -213.83                | NOP  |
> | *yjoB* — *rapA*   | 147                  | -380.85                | NOP  |
> | *ptsI* —— *splA*  | 93                   | -291.13                | NOP  |
>
> 
>
> ------

表[16.1](https://biopython-org.translate.goog/DIST/docs/tutorial/Tutorial.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN&_x_tr_pto=wapp&_x_tr_sch=http#table%3Atraining)列出了一些已知操纵子结构的*枯草芽孢杆菌基因对。*让我们根据这些数据计算逻辑回归模型：

```
>>> 从 Bio 导入 LogisticRegression
>>> xs = [
... [-53, -200.78],
... [117, -267.14],
... [57, -163.47],
... [16, -190.30],
... [11，-220.94]，
... [85, -193.94],
... [16, -182.71],
... [15, -180.41],
... [-26, -181.73],
... [58, -259.87],
... [126, -414.53],
... [191, -249.57],
... [113, -265.28],
... [145, -312.99],
... [154, -213.83],
... [147, -380.85],
... [93, -291.13],
...]
>>> ys = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
>>> 模型 = LogisticRegression.train(xs, ys)
```

这里，`xs`和`ys`是训练数据：`xs`包含每个基因对的预测变量，并`ys`指定基因对属于相同的操纵子（`1`，类 OP）还是不同的操纵子（`0`，类 NOP）。生成的逻辑回归模型存储在 中`model`，其中包含权重 β 0、β 1和 β 2：

```
>>> 模型.beta
[8.9830290157144681，-0.035968960444850887，0.02181395662983519]
```

请注意，β 1是负数，因为基因间距离越短的基因对属于同一操纵子（OP 类）的概率越高。另一方面，β 2是正的，因为属于同一操纵子的基因对通常具有较高的基因表达谱相似性得分。由于训练数据中操纵子基因对的普遍性高于非操纵子基因对，因此参数 β 0为正。

该函数`train`有两个可选参数：`update_fn`和`typecode`。可`update_fn`用于指定回调函数，将迭代次数和对数似然作为参数。使用回调函数，我们可以跟踪模型计算的进度（使用 Newton-Raphson 迭代来最大化逻辑回归模型的对数似然函数）：

```
>>> def show_progress（迭代，对数似然）：
...打印（f“迭代：{iteration}对数似然函数：{loglikelihood}”）
...
>>>
>>> 模型 = LogisticRegression.train(xs, ys, update_fn=show_progress)
迭代：0 对数似然函数：-11.7835020695
迭代：1 对数似然函数：-7.15886767672
迭代：2 对数似然函数：-5.76877209868
迭代：3 对数似然函数：-5.11362294338
迭代：4 对数似然函数：-4.74870642433
迭代：5 对数似然函数：-4.50026077146
迭代：6 对数似然函数：-4.31127773737
迭代：7 对数似然函数：-4.16015043396
迭代：8 对数似然函数：-4.03561719785
迭代：9 对数似然函数：-3.93073282192
迭代：10 对数似然函数：-3.84087660929
迭代：11 对数似然函数：-3.76282560605
迭代：12 对数似然函数：-3.69425027154
迭代：13 对数似然函数：-3.6334178602
迭代：14 对数似然函数：-3.57900855837
迭代：15 对数似然函数：-3.52999671386
迭代：16 对数似然函数：-3.48557145163
迭代：17 对数似然函数：-3.44508206139
迭代：18 对数似然函数：-3.40799948447
迭代：19 对数似然函数：-3.3738885624
迭代：20 对数似然函数：-3.3423876581
迭代：21 对数似然函数：-3.31319343769
迭代：22 对数似然函数：-3.2860493346
迭代：23 对数似然函数：-3.2607366863
迭代：24 对数似然函数：-3.23706784091
迭代：25 对数似然函数：-3.21488073614
迭代：26 对数似然函数：-3.19403459259
迭代：27 对数似然函数：-3.17440646052
迭代：28 对数似然函数：-3.15588842703
迭代：29 对数似然函数：-3.13838533947
迭代：30 对数似然函数：-3.12181293595
迭代：31 对数似然函数：-3.10609629966
迭代：32 对数似然函数：-3.09116857282
迭代：33 对数似然函数：-3.07696988017
迭代：34 对数似然函数：-3.06344642288
迭代：35 对数似然函数：-3.05054971191
迭代：36 对数似然函数：-3.03823591619
迭代：37 对数似然函数：-3.02646530573
迭代：38 对数似然函数：-3.01520177394
迭代：39 对数似然函数：-3.00441242601
迭代：40 对数似然函数：-2.99406722296
迭代：41 对数似然函数：-2.98413867259
```

一旦对数似然函数的增量小于 0.01，迭代就会停止。如果在 500 次迭代后仍未达到收敛，则`train`函数返回一个`AssertionError`.

optional 关键字`typecode`几乎总是可以被忽略。该关键字允许用户选择要使用的数值矩阵的类型。特别是，为了避免非常大的问题出现内存问题，可能需要使用单精度浮点数（Float8、Float16 等）而不是默认使用的双精度浮点数。

## 16.1.3 使用逻辑回归模型进行分类

通过调用函数进行分类`classify`。*给定逻辑回归模型和x* 1和*x* 2的值（例如，对于未知操纵子结构的基因对），该`classify`函数返回`1`或`0`，分别对应于类 OP 和类 NOP。例如，让我们考虑基因对*yxcE*、*yxcD*和*yxiB*、*yxiA*：

> ------
>
> 表 16.2：未知操纵子状态的相邻基因对。
>
> | 基因对          | 基因间距离*x* 1 | 基因表达评分*x* 2 |
> | --------------- | --------------- | ----------------- |
> | *yxcE* — *yxcD* | 6个             | -173.143442352    |
> | *yxiB* — *yxiA* | 309             | -271.005880394    |
>
> ------

逻辑回归模型将*yxcE*、*yxcD*分类为属于同一操纵子（类 OP），而预测*yxiB*、*yxiA*属于不同的操纵子：

```
>>> print("yxcE, yxcD:", LogisticRegression.classify(模型, [6, -173.143442352]))
yxcE, yxcD: 1
>>> print("yxiB, yxiA:", LogisticRegression.classify(模型, [309, -271.005880394]))
yxiB，yxiA：0
```

（顺便说一下，这与生物学文献一致）。

要了解我们对这些预测的信心程度，我们可以调用该函数来获取类 OP 和 NOP 的`calculate`概率（方程式 ( [16.2](https://biopython-org.translate.goog/DIST/docs/tutorial/Tutorial.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN&_x_tr_pto=wapp&_x_tr_sch=http#eq%3AOP) ) 和[16.3 ）。](https://biopython-org.translate.goog/DIST/docs/tutorial/Tutorial.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN&_x_tr_pto=wapp&_x_tr_sch=http#eq%3ANOP)对于*yxcE*，*yxcD*我们发现

```
>>> q, p = LogisticRegression.calculate(模型, [6, -173.143442352])
>>> print("类 OP: 概率 =", p, "类 NOP: 概率 =", q)
OP类：概率= 0.993242163503 NOP类：概率= 0.00675783649744
```

对于*yxiB* , *yxiA*

```
>>> q, p = LogisticRegression.calculate(模型, [309, -271.005880394])
>>> print("类 OP: 概率 =", p, "类 NOP: 概率 =", q)
OP类：概率= 0.000321211251817 NOP类：概率= 0.999678788748
```

为了了解逻辑回归模型的预测准确性，我们可以将其应用于训练数据：

```
>>> 对于范围内的我（len（ys））：
...打印（f“真：{ys [i]}预测：{LogisticRegression.classify（模型，xs [i]）}”）
...
真实：1 预测：1
真实：1 预测：0
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：0 预测：0
真实：0 预测：0
真实：0 预测：0
真实：0 预测：0
真实：0 预测：0
真实：0 预测：0
真实：0 预测：0
```

表明除了一对基因外，所有预测都是正确的。可以从留一法分析中找到更可靠的预测准确度估计，其中在删除要预测的基因后从训练数据中重新计算模型：

```
>>> 对于范围内的我（len（ys））：
...模型 = LogisticRegression.train(xs[:i] + xs[i + 1 :], ys[:i] + ys[i + 1 :])
...打印（f“真：{ys [i]}预测：{LogisticRegression.classify（模型，xs [i]）}”）
...
真实：1 预测：1
真实：1 预测：0
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：0 预测：0
真实：0 预测：0
真实：0 预测：0
真实：0 预测：0
真实：0 预测：1
真实：0 预测：0
真实：0 预测：0
```

留一法分析表明，逻辑回归模型的预测仅对其中两个基因对不正确，对应的预测准确率为 88%。

## 16.1.4 逻辑回归、线性判别分析和支持向量机

逻辑回归模型类似于线性判别分析。在线性判别分析中，类概率也遵循等式（[16.2](https://biopython-org.translate.goog/DIST/docs/tutorial/Tutorial.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN&_x_tr_pto=wapp&_x_tr_sch=http#eq%3AOP)）和（[16.3](https://biopython-org.translate.goog/DIST/docs/tutorial/Tutorial.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN&_x_tr_pto=wapp&_x_tr_sch=http#eq%3ANOP)）。*然而，我们不是直接估计系数 β，而是首先对预测变量x*拟合正态分布。然后根据正态分布的均值和协方差计算系数 β。*如果x*的分布确实是正态的，那么我们期望线性判别分析比逻辑回归模型表现更好。另一方面，逻辑回归模型对偏离正态性更稳健。

另一种类似的方法是具有线性内核的支持向量机。这样的 SVM 也使用预测变量的线性组合，但根据类之间边界区域附近的预测变量*x*估计系数 β 。如果逻辑回归模型（方程式（[16.2](https://biopython-org.translate.goog/DIST/docs/tutorial/Tutorial.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN&_x_tr_pto=wapp&_x_tr_sch=http#eq%3AOP)）和（[16.3](https://biopython-org.translate.goog/DIST/docs/tutorial/Tutorial.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN&_x_tr_pto=wapp&_x_tr_sch=http#eq%3ANOP)））很好地描述了远离边界区域的*x* ，我们期望逻辑回归模型比具有线性核的 SVM 表现更好，因为它依赖于更多数据. 如果不是，具有线性核的 SVM 可能表现更好。

Trevor Hastie、Robert Tibshirani 和 Jerome Friedman：*统计学习的要素。数据挖掘、推理和预测*。Springer 统计学系列，2001 年。第 4.4 章。

# 16.2  *k* - 最近邻

## 16.2.1 背景和目的

k最近邻方法是一种监督学习方法，不需要将模型拟合到数据*。*相反，数据点根据训练数据集中*k 个*最近邻居的类别进行分类。

在 Biopython 中，*k-*最近邻方法在`Bio.kNN`. *为了说明 Biopython 中k*最近邻方法的使用，我们将使用与[16.1](https://biopython-org.translate.goog/DIST/docs/tutorial/Tutorial.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN&_x_tr_pto=wapp&_x_tr_sch=http#sec%3ALogisticRegression)节中相同的操纵子数据集。

## 16.2.2 初始化*k-*最近邻模型

[使用表16.1](https://biopython-org.translate.goog/DIST/docs/tutorial/Tutorial.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN&_x_tr_pto=wapp&_x_tr_sch=http#table%3Atraining)中的数据，我们创建并初始化一个*k*最近邻模型，如下所示：

```
>>> 从生物导入 kNN
>>> k = 3
>>> 模型 = kNN.train(xs, ys, k)
```

其中`xs`和与[第 16.1.2](https://biopython-org.translate.goog/DIST/docs/tutorial/Tutorial.html?_x_tr_sl=auto&_x_tr_tl=zh-CN&_x_tr_hl=zh-CN&_x_tr_pto=wapp&_x_tr_sch=http#sec%3ALogisticRegressionTraining)`ys`节中的相同。这里，是将考虑用于分类的邻居数*k 。**对于分为两类的分类，为k*选择奇数可以避免平票。函数名称有点用词不当，因为没有进行模型训练：此函数只是存储、和in 。`k``train``xs``ys``k``model`

## 16.2.3 使用*k*最近邻模型进行分类

要使用*k*最近邻模型对新数据进行分类，我们使用`classify`函数。此函数采用数据点 ( *x* 1 , *x* 2 ) 并在训练数据集中找到*k 个*`xs`最近的邻居。然后根据*k 个*邻居中哪个类别 ( ) 出现最多，对数据点 ( *x* 1 , *x* 2 ) 进行分类。`ys`

以基因对*yxcE*、*yxcD*和*yxiB*、*yxiA*为例，我们发现：

```
>>> x = [6, -173.143442352]
>>> print("yxcE, yxcD:", kNN.classify(model, x))
yxcE, yxcD: 1
>>> x = [309, -271.005880394]
>>> print("yxiB, yxiA:", kNN.classify(model, x))
yxiB，yxiA：0
```

与逻辑回归模型一致，*yxcE*和*yxcD*被分类为属于同一操纵子（OP 类），而*yxiB*和*yxiA*被预测为属于不同的操纵子。

该`classify`函数允许我们将距离函数和权重函数指定为可选参数。距离函数会影响选择哪些*k 个*邻居作为最近的邻居，因为这些邻居被定义为与查询点 ( *x* , *y* ) 的距离最小的邻居。默认情况下，使用欧氏距离。相反，我们可以使用城市街区（曼哈顿）距离：

```
>>> def cityblock(x1, x2):
...断言 len(x1) == 2
...断言 len(x2) == 2
... 距离 = abs(x1[0] - x2[0]) + abs(x1[1] - x2[1])
...返回距离
...
>>> x = [6, -173.143442352]
>>> print("yxcE, yxcD:", kNN.classify(model, x, distance_fn=cityblock))
yxcE, yxcD: 1
```

权重函数可用于加权投票。例如，我们可能希望给予较近的邻居比距离较远的邻居更高的权重：

```
>>> 定义重量（x1，x2）：
...断言 len(x1) == 2
...断言 len(x2) == 2
...返回 exp(-abs(x1[0] - x2[0]) - abs(x1[1] - x2[1]))
...
>>> x = [6, -173.143442352]
>>> print("yxcE, yxcD:", kNN.classify(model, x, weight_fn=weight))
yxcE, yxcD: 1
```

默认情况下，所有邻居都被赋予相同的权重。

为了找出我们对这些预测的信心，我们可以调用该`calculate`函数，该函数将计算分配给类 OP 和 NOP 的总权重。对于默认的加权方案，这减少了每个类别中的邻居数量。对于*yxcE*，*yxcD*，我们发现

```
>>> x = [6, -173.143442352]
>>> weight = kNN.calculate(model, x)
>>> 打印（“类 OP：权重 =”，权重 [0]，“类 NOP：权重 =”，权重 [1]）
OP类：重量= 0.0 NOP类：重量= 3.0
```

这意味着 , 的所有三个邻居`x1`都`x2`在 NOP 类中。作为另一个例子，对于*yesK*，*yesL*我们发现

```
>>> x = [117, -267.14]
>>> weight = kNN.calculate(model, x)
>>> 打印（“类 OP：权重 =”，权重 [0]，“类 NOP：权重 =”，权重 [1]）
OP类：重量= 2.0 NOP类：重量= 1.0
```

这意味着两个邻居是操纵子对，一个邻居是非操纵子对。

为了了解*k*最近邻方法的预测准确性，我们可以将其应用于训练数据：

```
>>> 对于范围内的我（len（ys））：
...打印（f“真：{ys [i]}预测：{kNN.classify（模型，xs [i]）}”）
...
真实：1 预测：1
真实：1 预测：0
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：0
真实：0 预测：0
真实：0 预测：0
真实：0 预测：0
真实：0 预测：0
真实：0 预测：0
真实：0 预测：0
真实：0 预测：0
```

表明除了两个基因对之外，所有预测都是正确的。可以从留一法分析中找到更可靠的预测准确度估计，其中在删除要预测的基因后从训练数据中重新计算模型：

```
>>> k = 3
>>> 对于范围内的我（len（ys））：
...模型 = kNN.train(xs[:i] + xs[i + 1 :], ys[:i] + ys[i + 1 :], k)
...打印（f“真：{ys [i]}预测：{kNN.classify（模型，xs [i]）}”）
...
真实：1 预测：1
真实：1 预测：0
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：1
真实：1 预测：0
真实：0 预测：0
真实：0 预测：0
真实：0 预测：1
真实：0 预测：0
真实：0 预测：0
真实：0 预测：0
真实：0 预测：1
```

留一法分析表明，*k*最近邻模型对 17 个基因对中的 13 个是正确的，对应的预测准确率为 76%。

## 16.3 朴素贝叶斯

本节将介绍该`Bio.NaiveBayes`模块。

## 16.4 最大熵

本节将介绍该`Bio.MaxEntropy`模块。

## 16.5马尔可夫模型

本节将描述`Bio.MarkovModel`和/或`Bio.HMM.MarkovModel`模块。